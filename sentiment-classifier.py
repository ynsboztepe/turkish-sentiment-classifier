# -*- coding: utf-8 -*-
"""sentiment-classifier.ipynb

Automatically generated by Colab.

"""

# !pip install zeyrek nltk
import pandas as pd
import re
import zeyrek
import nltk
from nltk.corpus import stopwords
import logging
analyzer = zeyrek.MorphAnalyzer()
logging.getLogger('zeyrek').setLevel(logging.ERROR)
nltk.download("stopwords")
nltk.download('punkt_tab')
nltk.download('punkt')
stop_words = set(stopwords.words("turkish"))
df = pd.read_csv("/content/drive/MyDrive/NLP datasets/magaza_yorumlari.csv", encoding="utf-16")

"""# Veri Seti"""

df

"""# Etiket Dağılımı"""

print(df["Durum"].unique())
print(df["Durum"].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
ax = sns.countplot(data=df, x='Durum', palette='viridis')
plt.title('Etiket Dağılımı', fontsize=16)
plt.xlabel('Etiket', fontsize=12)
plt.ylabel('Veri Sayısı', fontsize=12)
plt.show()

"""# Ön İşleme"""

df.info()

print(df.isnull().sum())

df.dropna(inplace=True)

print(df.isnull().sum())

def temizle(metin):

  metin = metin.lower()
  metin = re.sub(r"[^\w\s]","",metin)
  kelimeler = [w for w in metin.split() if w not in stop_words]

  return " ".join(kelimeler)

def zeyrek_lemmatize(metin):

  kelimeler = metin.split()
  lemma_kelimeler = []

  for kelime in kelimeler:
      try:
          analizler = analyzer.analyze(kelime)

          if analizler:
              en_olasi_lemma = analizler[0][0].lemma

              if en_olasi_lemma == 'Unk':
                    lemma_kelimeler.append(kelime)
              else:
                    lemma_kelimeler.append(en_olasi_lemma.lower())
          else:
              lemma_kelimeler.append(kelime)
      except:
          lemma_kelimeler.append(kelime)

  return " ".join(lemma_kelimeler)

df["temiz_metin"] = df["Görüş"].apply(temizle)
df["lemma_metin"] = df["temiz_metin"].apply(zeyrek_lemmatize)

df.head()

df["kelime_sayisi"] = df["temiz_metin"].apply(lambda x: len(x.split()))
df["ortalama_kelime_uzunlugu"] = df["temiz_metin"].apply(
    lambda x: sum([len(w) for w in x.split()]) / len(x.split()) if len(x.split()) > 0 else 0
)
df["uzunluk"] = df["temiz_metin"].apply(len)
df["negatif_kelime_sayisi"] = df["temiz_metin"].apply(
    lambda x: len([w for w in x.split() if w in [ "kötü","berbat","rezalet","bozuk","yetersiz","geç","yavaş","kırık","hasarlı","kalitesiz",
                      "iğrenç","felaket","ilgisiz","pahalı","memnun değilim","hayal kırıklığı","eksik","arızalı",
                      "değmez","gelmedi","cevap vermiyor","aldatıcı","yanıltıcı","hüsran"]])
)

df

"""# TF-IDF

### Veri Sızıntısı (Data Leakage) yaşanmaması için TF-IDF hesaplaması train-test split işleminden sonra yapıldı.
"""

# !pip install scikit-learn -q
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack
import numpy as np

X = df[["temiz_metin", "kelime_sayisi", "ortalama_kelime_uzunlugu", "uzunluk", "negatif_kelime_sayisi"]]
y = df["Durum"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tfidf = TfidfVectorizer(max_features = 1000)
X_train_tfidf = tfidf.fit_transform(X_train["temiz_metin"])
X_test_tfidf = tfidf.transform(X_test["temiz_metin"])

train_sayisal = X_train[["kelime_sayisi", "ortalama_kelime_uzunlugu", "uzunluk", "negatif_kelime_sayisi"]]
test_sayisal = X_test[["kelime_sayisi", "ortalama_kelime_uzunlugu", "uzunluk", "negatif_kelime_sayisi"]]

X_train = hstack([X_train_tfidf, train_sayisal])
X_test = hstack([X_test_tfidf, test_sayisal])

"""# Model Eğitimi

# Accuracy, Precision, Recall, F1-score
"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

modeller = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(),
    "Support Vector Machine": LinearSVC()
}


sonuclar = []
for isim, model in modeller.items():
  model.fit(X_train, y_train)
  tahmin = model.predict(X_test)
  accuracy =accuracy_score(y_test, tahmin)
  precision = precision_score(y_test, tahmin, pos_label = "Olumlu")
  recall = recall_score(y_test, tahmin, pos_label = "Olumlu")
  f1 = f1_score (y_test, tahmin, pos_label = "Olumlu")
  sonuclar.append({
      "Model": isim,
      "Accuracy": accuracy,
      "Precision": precision,
      "Recall": recall,
      "F1 Score": f1
  })
  print(isim)
  print(30*"-")
  print(classification_report(y_test, tahmin))
  cm = confusion_matrix(y_test, tahmin, labels=['Olumlu', 'Olumsuz'])
  plt.figure(figsize=(6, 5))
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
              xticklabels=['Tahmin Pozitif', 'Tahmin Negatif'],
              yticklabels=['Gerçek Pozitif', 'Gerçek Negatif'])
  plt.title(f'{isim} Karışıklık Matrisi')
  plt.ylabel('Gerçek Etiket')
  plt.xlabel('Tahmin Edilen Etiket')
  plt.show()
  print(30*"-")

sonucDf = pd.DataFrame(sonuclar)
print(sonucDf.round(3))

"""# Performans karşılaştırma grafiği"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20,6))

df_melt = sonucDf.melt(id_vars="Model", var_name="Metrik", value_name="Değer")
ax =sns.barplot(x="Model", y="Değer", hue="Metrik", data=df_melt)

for p in ax.patches:
  value = p.get_height()
  if not pd.isna(value):
    if value > 0:
      ax.text(
          p.get_x() + p.get_width() / 2,
          value + 0.2,
          f"{value:.2f}",
          ha="center",
          va="bottom"
      )
plt.title("Duygu Analizi Sonuçları")
plt.legend(title = "Metrik", loc="best")
plt.xlabel("Model")
plt.ylabel("Değer")
plt.show()